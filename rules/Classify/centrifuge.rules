localrules:
    krona_taxonomy,
    extract_centrifuge_sequences,
    extract_centrifuge_seqidmap,
    centrifuge2krona,
    centrifuge_kreport,
    all_centrifuge_to_krona,
    centrifuge_filter

#####################################
## Prepare taxonomy database files ##
#####################################

rule krona_taxonomy:
    output:
        opj(config["taxdb"],"krona","taxonomy.tab")
    params:
        taxdir = opj(config["taxdb"],"taxonomy")
    shadow: "minimal"
    shell:
        """
        ktUpdateTaxonomy.sh --only-build --preserve {params.taxdir}
        mv {params.taxdir}/taxonomy.tab {output}
        touch {output}
        """

rule sqlite_taxdb:
    """Creates a sqlite database for ete3 in the centrifuge path"""
    output:
        opj(config["taxdb"],"taxonomy","taxdb.sqlite"),
        opj(config["taxdb"],"taxonomy","taxdb.sqlite.traverse.pkl")
    message: "Creating ete3 sqlite taxonomy database in {output[0]}"
    shadow: "minimal"
    run:
        from ete3 import NCBITaxa
        shell("touch {output[0]}")
        ncbi_taxa = NCBITaxa(output[0])

#################################################
## Centrifuge database sequences and seqid2map ##
#################################################

rule extract_centrifuge_sequences:
    """Extracts input sequences from centrifuge database"""
    input:
        db = expand(opj(config["centrifuge_dir"], "{base}.{i}.cf"),
            i=[1,2,3], base=config['centrifuge_base'])
    output:
        fna = opj(config["centrifuge_dir"], "input-sequences.fna.gz")
    params:
        prefix = opj(config["centrifuge_dir"], "{base}".format(base=config["centrifuge_base"]))
    run:
        unzipped = (output.fna).replace(".gz", "")
        if os.path.exists(output.fna):
            pass
        elif os.path.exists(unzipped):
            shell("gzip -c {unzipped} > {output.fna}")
        else:
            shell("centrifuge-inspect {params.prefix} | gzip -c > {output.fna}")

rule extract_centrifuge_seqidmap:
    """Extracts sequence id to taxonomy idmapping from centrifuge database"""
    input:
        db = expand(opj(config["centrifuge_dir"], "{base}.{i}.cf"), i=[1,2,3], base=config["centrifuge_base"])
    output:
        map = opj(config["centrifuge_dir"], "seqid2taxid.map")
    params:
        prefix = opj(config["centrifuge_dir"], "{base}".format(base=config["centrifuge_base"]))
    run:
        if not os.path.exists(output.map):
            shell(" centrifuge-inspect --conversion-table {params.prefix} > {output.map} ")
        

############################
## Centrifuge classifying ##
############################

rule centrifuge_map_se:
    input:
        opj(config["data_path"],"{sample}_se.fq"),
        expand(opj(config["centrifuge_dir"],"{base}.{i}.cf"), i=[1,2,3], base=config["centrifuge_base"])
    output:
        opj(config["results_path"],"centrifuge","{sample}_se.out"),
        opj(config["results_path"],"centrifuge","{sample}_se.report.tsv")
    params:
        prefix = opj(config["centrifuge_dir"], "{base}".format(base=config["centrifuge_base"])),
        tmp_out = opj(config["scratch_path"],"{sample}_se.out"),
        tmp_report = opj(config["scratch_path"],"{sample}_se.report.tsv")
    threads: 8
    resources:
        runtime = lambda wildcards, attempt: attempt**2*60
    message: "Running centrifuge on {wildcards.sample}"
    shell:
        """
        mkdir -p {config[scratch_path]}
        centrifuge -k {config[centrifuge_max_assignments]} -U {input[0]} -x {params.prefix} -S {params.tmp_out} \
         --report-file {params.tmp_report} -p {threads}
        mv {params.tmp_out} {output[0]}
        mv {params.tmp_report} {output[1]}
        """

#######################
## Remove host reads ##
#######################

rule filter_out_host:
    input:
        opj(config["data_path"],"{sample}_se.fq"),
        opj(config["results_path"],"centrifuge","{sample}_se.out")
    output:
        temp(opj(config["results_path"],"centrifuge","{sample}_se.filtered.read.ids")),
        opj(config["results_path"],"centrifuge","{sample}_se.filtered.fq")
    params:
        host_taxid = config['host_taxid']
    threads: 8
    resources:
        runtime = lambda wildcards, attempt: attempt**2*60
    message: "Removing host reads on {wildcards.sample}"
    shell:
        """
        cut -f 1,2,3 {input[1]} | grep -v $'\t'{params.host_taxid}$ | cut -f 1 > {output[0]}
        seqtk subseq {input[0]} {output[0]} > {output[1]}
        """

#####################################
## Generate kreports/krona reports ##
#####################################

rule centrifuge_kreport:
    input:
        f = opj(config["results_path"],"centrifuge","{sample}_se.out"),
        db = expand(opj(config["centrifuge_dir"],"{base}.{i}.cf"), i=[1,2,3], base=config["centrifuge_base"])
    output:
        opj(config["results_path"],"centrifuge","{sample}_se.report")
    params:
        min_score = config["centrifuge_min_score"],
        prefix = opj(config["centrifuge_dir"], "{base}".format(base=config["centrifuge_base"])),
    shell:
        """
        centrifuge-kreport --min-score {params.min_score} -x {params.prefix} {input.f} > {output[0]}
        """

rule centrifuge2krona:
    input:
        opj(config["results_path"],"centrifuge","{sample}_se.report"),
        opj(config["taxdb"],"krona","taxonomy.tab")
    output:
        opj(config["results_path"],"centrifuge","{sample}_se.krona.html")
    params:
        tax = opj(config["taxdb"],"krona")
    shell:
        """
        ktImportTaxonomy -m 3 -t 5 -tax {params.tax} -o {output[0]} {input[0]},{wildcards.sample}
        """

rule all_centrifuge_to_krona:
    input:
        f = expand(opj(config["results_path"],"centrifuge", "{sample}_se.report"),sample=Samples.keys()),
        h = expand(opj(config["results_path"],"centrifuge", "{sample}_se.html"),sample=Samples.keys()),
        t = opj(config["taxdb"],"krona","taxonomy.tab")
    output:
        opj(config["report_path"],"centrifuge","centrifuge.krona.html")
    params:
        tax = opj(config["taxdb"],"krona")
    run:
        input_string = ""
        for f in input.f:
            sample_run = os.path.basename(f).replace("_se.report","")
            print(sample_run,f)
            input_string+=" {},{}".format(f,sample_run)
        shell("ktImportTaxonomy -t 5 -m 3 -tax {params.tax} -o {output[0]} {input_string}")


###########################################################
## Filter out genome sequences based on defined cutoffs, ##
## e.g., min_read_count,min_abundance; otherwise, keep   ##
## only the most abundant one and non-host one with      ##
## most reads                                            ##
###########################################################

rule centrifuge_filter:
    """Filter genomes by centrifuge read counts or abundance"""
    input:
        opj(config["results_path"],"centrifuge","{sample}_se.report.tsv"),
        opj(config["taxdb"],"taxonomy","taxdb.sqlite"),
        opj(config["centrifuge_dir"],"seqid2taxid.map")
    output:
        opj(config["results_path"],"centrifuge","{sample}_se.filtered_genomes")
    params:
        min_read_count = int(config["centrifuge_min_read_count"]),
        min_abundance = float(config["centrifuge_min_abundance"]),
        host_taxid = str(config["host_taxid"])
    script:
        "../../scripts/centrifuge_filter_genomes.py"

rule get_all_filtered_genomes:
    """Extracts nucleotide fasta files of the filtered genomes for all samples"""
    input:
        genome_files = expand(opj(config["results_path"],"centrifuge","{sample}_se.filtered_genomes"), sample=Samples.keys()),
        fna = opj(config["centrifuge_dir"],"input-sequences.fna.gz")
    output:
        fna = temp(opj(config["results_path"],"centrifuge","filtered","genomes.fna"))
    params:
        out_dir = opj(config["results_path"],"centrifuge","filtered"),
        seqids = temp(opj(config["results_path"],"centrifuge","filtered","seqids"))
    run:
        shell("mkdir -p {params.out_dir}")
        df = pd.DataFrame()
        for f in input.genome_files:

            # when no candidate species passes the above filtering process,
            # an empty {sample}_se.filtered_genomes will be generated with file size of 12 (only rownames),
            # then we skip it
        
            if os.stat(f).st_size == 12:
                pass
                
            else:
                _df = pd.read_csv(f, sep="\t")
                df = pd.concat([df,_df], sort=True)
                # Make unique by seqid
                df = df.groupby("seq").first().reset_index()
                # Write seqids to file
                with open(params.seqids, 'w') as fhout:
                    for seqid in df.seq:
                        fhout.write("{}\n".format(seqid))

        # Extract sequences
        if os.path.exists(params.seqids):
            shell("seqtk subseq {input.fna} {params.seqids} > {output.fna}")
            #df.to_csv(output.tab, sep="\t", index=False, header=True)
            shell("rm {params.seqids}")
        else:
            open(output.fna,'a').close()

rule get_filtered_genomes:
    """Extracts nucleotide fasta files of the filtered genomes for each sample"""
    input:
        opj(config["results_path"],"centrifuge","filtered","genomes.fna"),
        opj(config["results_path"],"centrifuge","{sample}_se.filtered_genomes")
    output:
        fna = opj(config["results_path"],"centrifuge","filtered","{sample}_se.genomes.fna")
    params:
        seqids = temp(opj(config["results_path"],"centrifuge","filtered","{sample}_se.seqids"))
    resources:
        runtime = lambda wildcards, attempt: attempt**2*60*2
    run:
        # when no candidate species passes the above filtering process,
        # an empty {sample}_se.filtered_genomes will be generated with file size of 12 (only rownames).
        # we remove this file and exit the program to the system.
        if os.path.getsize(input[1]) == 12: ## | os.path.getsize(input[0]) == 0:
            #basedir = os.path.dirname(params.bam)
            #if not os.path.exists(basedir):
            #    os.makedirs(basedir)
            #open(params.bam,'a').close()

            print("No genomes were found on " + input[1] + ". Pls. set less stringent parameters.\n")
            os.remove(input[1])
            sys.exit(0)
        else:
            df = pd.DataFrame()
            _df = pd.read_csv(input[1], sep="\t")
            df = pd.concat([df,_df], sort=True)
            # Make unique by seqid
            df = df.groupby("seq").first().reset_index()
            # Write seqids to file
            with open(params.seqids, 'w') as fhout:
                for seqid in df.seq:
                    fhout.write("{}\n".format(seqid))

            # Extract sequences to individual sample
            shell("seqtk subseq {input[0]} {params.seqids} > {output.fna}")
