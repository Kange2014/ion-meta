###########################################
## Filter genomes further using sourmash ##
###########################################

rule define_sourmash_hash_fraction:
    input:
        opj(config["results_path"],"centrifuge","{sample}_se.filtered.fq")
    output:
        opj(config["results_path"],"centrifuge","{sample}_se.sourmash_hash_fraction.txt")
    params:
        sample = lambda wildcards: wildcards.sample
    run:
        count = -1
        for count,line in enumerate(open(input[0],'r')):
            pass
        count += 1
    
        if count >= int(config["sourmash_fraction_num_reads"])*4:
            sourmash_fraction = 1000
        else:
            sourmash_fraction = 100
    
        #config[params.sample] = sourmash_fraction

        with open(output[0],'w') as f:
            f.write(str(sourmash_fraction))
        

rule sourmash_hash_genomes:
    """Compute k-mer signatures for filtered genomes"""
    input:
        opj(config["results_path"],"centrifuge","filtered","{sample}_se.genomes.fna"),
        opj(config["results_path"],"centrifuge","{sample}_se.sourmash_hash_fraction.txt")
    output:
        sig = opj(config["results_path"],"sourmash","{sample}_se.genomes.sig")
    resources:
        runtime = lambda wildcards, attempt: attempt**2*60*2
    shell:
        """
        hash_fraction=`cat {input[1]}`
        sourmash compute --singleton -k 31 --scaled $hash_fraction -o {output.sig} -f {input[0]}
        """

rule sourmash_hash_sample_se:
    """Compute k-mer signatures for single-end samples"""
    input:
        opj(config["results_path"],"centrifuge","{sample}_se.filtered.fq"),
        opj(config["results_path"],"centrifuge","{sample}_se.sourmash_hash_fraction.txt")
    output:
        opj(config["results_path"],"sourmash","{sample}_se.sig")
    resources:
        runtime = lambda wildcards, attempt: attempt**2*60*2
    shell:
        """
        hash_fraction=`cat {input[1]}`
        sourmash compute -k 31 --scaled $hash_fraction --merge {wildcards.sample} \
        -o {output[0]} {input[0]}
        """

rule sourmash_coverage:
    """Calculate coverage of filtered genomes for each sample"""
    input:
        sample = opj(config["results_path"],"sourmash","{sample}_se.sig"),
        genome = opj(config["results_path"],"sourmash","{sample}_se.genomes.sig")
    output:
        opj(config["results_path"],"sourmash","{sample}_se.sourmash.csv")
    resources:
        runtime = lambda wildcards, attempt: attempt**2*60

    ### --threshold-bp 100 because in viral metagenomics, sometimes only several hundred bp overlap could be identified
    shell:
        """
        sourmash gather --threshold-bp 100 -o {output[0]} -k 31 {input.sample} {input.genome}
        """

#rule collate_sourmash:
#    """Collate all sourmash coverage files"""
#    input:
#        expand(opj(config["results_path"],"sourmash","{sample}_se.csv"),sample=Samples.keys())
#    output:
#        opj(config["results_path"],"centrifuge","filtered","sourmash.csv")
#    run:
#        df = pd.DataFrame()
#        for f in input:
#            sample_run = os.path.basename(f).rstrip(".csv")
#            _df = pd.read_csv(f, header=0)
#            _df = _df.assign(sample=pd.Series([sample_run]*len(_df)))
#            df = pd.concat([df,_df])
#        df.set_index("sample")
#        df.to_csv(output[0], index=True, header=True)

rule sourmash_filter:
    """Reads results from sourmash and outputs a fasta file with genomes reaching a certain coverage threshold"""
    input:
        opj(config["results_path"],"sourmash","{sample}_se.sourmash.csv"),
        opj(config["results_path"],"centrifuge","filtered","{sample}_se.genomes.fna"),
        opj(config["taxdb"],"taxonomy","taxdb.sqlite"),
        opj(config["results_path"],"centrifuge","{sample}_se.filtered_genomes"),
        opj(config["results_path"],"centrifuge","{sample}_se.report.tsv")
    output:
        opj(config["results_path"],"centrifuge","filtered","{sample}_se.genomes.filtered.fna"),
        opj(config["results_path"],"centrifuge","filtered","{sample}_se.genomes.filtered.ids.tax"),
        temp(opj(config["results_path"],"centrifuge","filtered","{sample}_se.genomes.filtered.ids"))
    params:
        min_cov = config["sourmash_min_cov"]
    run:
        from ete3 import NCBITaxa
        ncbi = NCBITaxa(input[2])

        df = pd.read_csv(input[0])
        temp = df.loc[df.f_match>=params.min_cov]
        
        seqid2taxid = pd.read_csv(input[3], sep="\t")
        # Make unique by seqid
        seqid2taxid = seqid2taxid.groupby("seq").first().reset_index()

        # if no genomes meet coverage requirement, then all candidate genomes are used for mapping
        if temp.empty:
            pass
        else: 
            df = temp
        
        ids = list(set([x.split(" ")[0] for x in df.name]))
        
        taxids = seqid2taxid.loc[seqid2taxid['seq'].isin(ids)].taxID
        taxids = list(set(taxids))
        taxnames = pd.DataFrame(columns=["taxID","Name","Genus","Family","Kingdom"])
        for taxid in taxids:
            lineage = ncbi.get_lineage(taxid)
            ranks = ncbi.get_rank(lineage)

            genus_name = ""
            family_name = ""
            kingdom_name = ""
            for key, value in ranks.items():
                if(value == "genus"): genus_name = ncbi.get_taxid_translator([key]).get(key)
                if(value == "family"): family_name = ncbi.get_taxid_translator([key]).get(key)
                if(value == "superkingdom"): kingdom_name = ncbi.get_taxid_translator([key]).get(key)
            
            tmp = pd.Series([taxid,ncbi.get_taxid_translator([taxid]).get(taxid),genus_name,family_name,kingdom_name],index=["taxID","Name","Genus","Family","Kingdom"])
            taxnames = taxnames.append(tmp,ignore_index=True)
        
        ids2tax = pd.merge(seqid2taxid,taxnames,on="taxID")
        ids2tax = ids2tax.groupby("seq").first().reset_index()
        ids2tax.to_csv(output[1],sep="\t",index=False)

        with open(output[2], 'w') as fhout:
            for id in ids:
                fhout.write("{}\n".format(id))
        shell("seqtk subseq {input[1]} {output[2]} > {output[0]}")
